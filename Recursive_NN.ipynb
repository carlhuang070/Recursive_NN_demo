{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carl after forward :\n",
      "[[ 0.00094135]\n",
      " [ 0.00016205]]\n",
      "carl for test children_delta:\n",
      "[[ -1.64596059e-04]\n",
      " [ -1.91773818e-04]\n",
      " [ -1.16616184e-05]\n",
      " [  1.93623263e-04]]\n",
      "slices:\n",
      "[(0, 0, 2), (1, 2, 4)]\n",
      "carl for test children_delta:\n",
      "[[  2.94946247e-08]\n",
      " [  3.41951937e-08]\n",
      " [  4.25121796e-09]\n",
      " [ -3.45468802e-08]]\n",
      "slices:\n",
      "[(0, 0, 2), (1, 2, 4)]\n",
      "carl test parent grad\n",
      "[[  3.36666681e-04  -1.45302180e-04   5.00000000e+00   6.00000000e+00]\n",
      " [  3.36666681e-04  -1.45302180e-04   5.00000000e+00   6.00000000e+00]] [[ 1.]\n",
      " [ 1.]]\n",
      "carl test parent grad\n",
      "[[-0.0001646  -0.00032919 -0.00049379 -0.00065838]\n",
      " [-0.00019177 -0.00038355 -0.00057532 -0.0007671 ]] [[-0.0001646 ]\n",
      " [-0.00019177]]\n",
      "weights(0,0): expected - actural 1.7207e-04 - 1.7207e-04\n",
      "weights(0,1): expected - actural -4.7449e-04 - -4.7449e-04\n",
      "weights(0,2): expected - actural 4.9995e+00 - 4.9995e+00\n",
      "weights(0,3): expected - actural 5.9993e+00 - 5.9993e+00\n",
      "weights(1,0): expected - actural 1.4489e-04 - 1.4489e-04\n",
      "weights(1,1): expected - actural -5.2885e-04 - -5.2885e-04\n",
      "weights(1,2): expected - actural 4.9994e+00 - 4.9994e+00\n",
      "weights(1,3): expected - actural 5.9992e+00 - 5.9992e+00\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "import numpy as np\n",
    "#from cnn import IdentityActivator\n",
    "\n",
    "class IdentityActivator(object):\n",
    "    def forward(self, weighted_input):\n",
    "        return weighted_input\n",
    "    def backward(self, output):\n",
    "        return 1\n",
    "    \n",
    "    \n",
    "class TreeNode(object):\n",
    "    def __init__(self, data, children=[], children_data=[]):\n",
    "        self.parent = None\n",
    "        self.children = children\n",
    "        self.children_data = children_data\n",
    "        self.data = data\n",
    "        if self.children:\n",
    "            for child in children:\n",
    "                child.parent = self\n",
    "            \n",
    "            \n",
    "# 递归神经网络实现\n",
    "class RecursiveLayer(object):\n",
    "    def __init__(self, node_width, child_count, \n",
    "                 activator, learning_rate):\n",
    "        '''\n",
    "        递归神经网络构造函数\n",
    "        node_width: 表示每个节点的向量的维度\n",
    "        child_count: 每个父节点有几个子节点\n",
    "        activator: 激活函数对象\n",
    "        learning_rate: 梯度下降算法学习率\n",
    "        '''\n",
    "        self.node_width = node_width\n",
    "        self.child_count = child_count\n",
    "        self.activator = activator\n",
    "        self.learning_rate = learning_rate\n",
    "        # 权重数组W\n",
    "        self.W = np.random.uniform(-1e-4, 1e-4,\n",
    "            (node_width, node_width * child_count))\n",
    "        # 偏置项b\n",
    "        self.b = np.zeros((node_width, 1))\n",
    "        # 递归神经网络生成的树的根节点\n",
    "        self.root = None\n",
    "        \n",
    "        \n",
    "    def forward(self, *children):\n",
    "        '''\n",
    "        前向计算\n",
    "        '''\n",
    "    \n",
    "        children_data = self.concatenate(children)\n",
    " \n",
    "        parent_data = self.activator.forward(\n",
    "            np.dot(self.W, children_data) + self.b\n",
    "        )\n",
    "\n",
    "        self.root = TreeNode(parent_data, children,children_data)\n",
    "       \n",
    "        \n",
    "        \n",
    "    def concatenate(self, tree_nodes):\n",
    "        '''\n",
    "        将各个树节点中的数据拼接成一个长向量\n",
    "        '''\n",
    "        concat = np.zeros((0,1))\n",
    "        for node in tree_nodes:\n",
    "            concat = np.concatenate((concat, node.data))\n",
    "        return concat\n",
    "    \n",
    "    \n",
    "    \n",
    "    def backward(self, parent_delta):\n",
    "        '''\n",
    "        BPTS反向传播算法\n",
    "        '''\n",
    "        self.calc_delta(parent_delta, self.root)\n",
    "        self.W_grad, self.b_grad = self.calc_gradient(self.root)\n",
    "    def calc_delta(self, parent_delta, parent):\n",
    "        '''\n",
    "        计算每个节点的delta\n",
    "        '''\n",
    "        parent.delta = parent_delta\n",
    "        if parent.children:\n",
    "            # 根据式2计算每个子节点的delta\n",
    "            children_delta = np.dot(self.W.T, parent_delta) * (\n",
    "                self.activator.backward(parent.children_data)\n",
    "            )\n",
    "            print(\"carl for test children_delta:\")\n",
    "            print(children_delta)\n",
    "            # slices = [(子节点编号，子节点delta起始位置，子节点delta结束位置)]\n",
    "            slices = [(i, i * self.node_width, \n",
    "                        (i + 1) * self.node_width)\n",
    "                        for i in range(self.child_count)]\n",
    "            \n",
    "            print(\"slices:\")\n",
    "            print(slices)\n",
    "            \n",
    "            # 针对每个子节点，递归调用calc_delta函数\n",
    "            for s in slices:\n",
    "                self.calc_delta(children_delta[s[1]:s[2]], \n",
    "                                parent.children[s[0]])\n",
    "    def calc_gradient(self, parent):\n",
    "        '''\n",
    "        计算每个节点权重的梯度，并将它们求和，得到最终的梯度\n",
    "        '''\n",
    "        W_grad = np.zeros((self.node_width, \n",
    "                            self.node_width * self.child_count))\n",
    "        b_grad = np.zeros((self.node_width, 1))\n",
    "        if not parent.children:\n",
    "            return W_grad, b_grad\n",
    "        parent.W_grad = np.dot(parent.delta, parent.children_data.T)\n",
    "        parent.b_grad = parent.delta\n",
    "        print(\"carl test parent grad\")\n",
    "        print(parent.W_grad,parent.b_grad)\n",
    "        \n",
    "        W_grad += parent.W_grad\n",
    "        b_grad += parent.b_grad\n",
    "        for child in parent.children:\n",
    "            W, b = self.calc_gradient(child)\n",
    "            W_grad += W\n",
    "            b_grad += b\n",
    "        return W_grad, b_grad\n",
    "    \n",
    "    \n",
    "    \n",
    "    def update(self):\n",
    "        '''\n",
    "        使用SGD算法更新权重\n",
    "        '''\n",
    "        self.W -= self.learning_rate * self.W_grad\n",
    "        self.b -= self.learning_rate * self.b_grad\n",
    "        \n",
    "\n",
    "    def reset_state(self):  \n",
    "        self.root = None\n",
    "        \n",
    "        \n",
    "def data_set():\n",
    "    x = [TreeNode([[1],[2]],None,None),\n",
    "         TreeNode([[3],[4]],None,None),\n",
    "         TreeNode([[5],[6]],None,None)\n",
    "        ]\n",
    "    d = np.array([[1], [2]])\n",
    "    return x, d        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "def gradient_check():\n",
    "    '''\n",
    "    梯度检查\n",
    "    '''\n",
    "    # 设计一个误差函数，取所有节点输出项之和\n",
    "    error_function = lambda o: o.sum()\n",
    "    rnn = RecursiveLayer(2, 2, IdentityActivator(), 1e-3)\n",
    "    # 计算forward值\n",
    "    x, d = data_set()\n",
    "    rnn.forward(x[0], x[1])\n",
    "    rnn.forward(rnn.root, x[2])\n",
    "    \n",
    "    print(\"carl after forward :\")\n",
    "    print(rnn.root.data)\n",
    "    # 求取sensitivity map\n",
    "    sensitivity_array = np.ones((rnn.node_width, 1),\n",
    "                                dtype=np.float64)\n",
    "    \n",
    "    # 计算梯度\n",
    "    rnn.backward(sensitivity_array)\n",
    "    # 检查梯度\n",
    "    epsilon = 10e-4\n",
    "    for i in range(rnn.W.shape[0]):\n",
    "        for j in range(rnn.W.shape[1]):\n",
    "            rnn.W[i,j] += epsilon\n",
    "            rnn.reset_state()\n",
    "            rnn.forward(x[0], x[1])\n",
    "            rnn.forward(rnn.root, x[2])\n",
    "            err1 = error_function(rnn.root.data)\n",
    "            rnn.W[i,j] -= 2*epsilon\n",
    "            rnn.reset_state()\n",
    "            rnn.forward(x[0], x[1])\n",
    "            rnn.forward(rnn.root, x[2])\n",
    "            err2 = error_function(rnn.root.data)\n",
    "            expect_grad = (err1 - err2) / (2 * epsilon)\n",
    "            rnn.W[i,j] += epsilon\n",
    "            print ('weights(%d,%d): expected - actural %.4e - %.4e' % (\n",
    "                i, j, expect_grad, rnn.W_grad[i,j]))\n",
    "    return rnn\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gradient_check()\n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
